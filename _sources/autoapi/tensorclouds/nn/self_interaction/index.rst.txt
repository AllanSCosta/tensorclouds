tensorclouds.nn.self_interaction
================================

.. py:module:: tensorclouds.nn.self_interaction


Classes
-------

.. autoapisummary::

   tensorclouds.nn.self_interaction.FullTensorSquareSelfInteraction
   tensorclouds.nn.self_interaction.ChannelWiseTensorSquareSelfInteraction
   tensorclouds.nn.self_interaction.SegmentedTensorSquareSelfInteraction
   tensorclouds.nn.self_interaction.SelfInteraction


Module Contents
---------------

.. py:class:: FullTensorSquareSelfInteraction

   Bases: :py:obj:`flax.linen.Module`


   Base class for all neural network modules.

   Layers and models should subclass this class.

   All Flax Modules are Python 3.7
   `dataclasses <https://docs.python.org/3/library/dataclasses.html>`_. Since
   dataclasses take over ``__init__``, you should instead override :meth:`setup`,
   which is automatically called to initialize the module.

   Modules can contain submodules, and in this way can be nested in a tree
   structure. Submodels can be assigned as regular attributes inside the
   :meth:`setup` method.

   You can define arbitrary "forward pass" methods on your Module subclass.
   While no methods are special-cased, ``__call__`` is a popular choice because
   it allows you to use module instances as if they are functions::

     >>> from flax import linen as nn
     >>> from typing import Tuple

     >>> class Module(nn.Module):
     ...   features: Tuple[int, ...] = (16, 4)

     ...   def setup(self):
     ...     self.dense1 = nn.Dense(self.features[0])
     ...     self.dense2 = nn.Dense(self.features[1])

     ...   def __call__(self, x):
     ...     return self.dense2(nn.relu(self.dense1(x)))

   Optionally, for more concise module implementations where submodules
   definitions are co-located with their usage, you can use the
   :meth:`compact` wrapper.


   .. py:attribute:: irreps
      :type:  e3nn_jax.Irreps


   .. py:attribute:: norm
      :type:  bool
      :value: True



.. py:class:: ChannelWiseTensorSquareSelfInteraction

   Bases: :py:obj:`flax.linen.Module`


   Base class for all neural network modules.

   Layers and models should subclass this class.

   All Flax Modules are Python 3.7
   `dataclasses <https://docs.python.org/3/library/dataclasses.html>`_. Since
   dataclasses take over ``__init__``, you should instead override :meth:`setup`,
   which is automatically called to initialize the module.

   Modules can contain submodules, and in this way can be nested in a tree
   structure. Submodels can be assigned as regular attributes inside the
   :meth:`setup` method.

   You can define arbitrary "forward pass" methods on your Module subclass.
   While no methods are special-cased, ``__call__`` is a popular choice because
   it allows you to use module instances as if they are functions::

     >>> from flax import linen as nn
     >>> from typing import Tuple

     >>> class Module(nn.Module):
     ...   features: Tuple[int, ...] = (16, 4)

     ...   def setup(self):
     ...     self.dense1 = nn.Dense(self.features[0])
     ...     self.dense2 = nn.Dense(self.features[1])

     ...   def __call__(self, x):
     ...     return self.dense2(nn.relu(self.dense1(x)))

   Optionally, for more concise module implementations where submodules
   definitions are co-located with their usage, you can use the
   :meth:`compact` wrapper.


   .. py:attribute:: irreps
      :type:  e3nn_jax.Irreps


   .. py:attribute:: norm
      :type:  bool
      :value: True



.. py:class:: SegmentedTensorSquareSelfInteraction

   Bases: :py:obj:`flax.linen.Module`


   Base class for all neural network modules.

   Layers and models should subclass this class.

   All Flax Modules are Python 3.7
   `dataclasses <https://docs.python.org/3/library/dataclasses.html>`_. Since
   dataclasses take over ``__init__``, you should instead override :meth:`setup`,
   which is automatically called to initialize the module.

   Modules can contain submodules, and in this way can be nested in a tree
   structure. Submodels can be assigned as regular attributes inside the
   :meth:`setup` method.

   You can define arbitrary "forward pass" methods on your Module subclass.
   While no methods are special-cased, ``__call__`` is a popular choice because
   it allows you to use module instances as if they are functions::

     >>> from flax import linen as nn
     >>> from typing import Tuple

     >>> class Module(nn.Module):
     ...   features: Tuple[int, ...] = (16, 4)

     ...   def setup(self):
     ...     self.dense1 = nn.Dense(self.features[0])
     ...     self.dense2 = nn.Dense(self.features[1])

     ...   def __call__(self, x):
     ...     return self.dense2(nn.relu(self.dense1(x)))

   Optionally, for more concise module implementations where submodules
   definitions are co-located with their usage, you can use the
   :meth:`compact` wrapper.


   .. py:attribute:: irreps
      :type:  e3nn_jax.Irreps


   .. py:attribute:: norm
      :type:  bool
      :value: True



   .. py:attribute:: segment_size
      :value: 2



.. py:class:: SelfInteraction

   Bases: :py:obj:`flax.linen.Module`


   Base class for all neural network modules.

   Layers and models should subclass this class.

   All Flax Modules are Python 3.7
   `dataclasses <https://docs.python.org/3/library/dataclasses.html>`_. Since
   dataclasses take over ``__init__``, you should instead override :meth:`setup`,
   which is automatically called to initialize the module.

   Modules can contain submodules, and in this way can be nested in a tree
   structure. Submodels can be assigned as regular attributes inside the
   :meth:`setup` method.

   You can define arbitrary "forward pass" methods on your Module subclass.
   While no methods are special-cased, ``__call__`` is a popular choice because
   it allows you to use module instances as if they are functions::

     >>> from flax import linen as nn
     >>> from typing import Tuple

     >>> class Module(nn.Module):
     ...   features: Tuple[int, ...] = (16, 4)

     ...   def setup(self):
     ...     self.dense1 = nn.Dense(self.features[0])
     ...     self.dense2 = nn.Dense(self.features[1])

     ...   def __call__(self, x):
     ...     return self.dense2(nn.relu(self.dense1(x)))

   Optionally, for more concise module implementations where submodules
   definitions are co-located with their usage, you can use the
   :meth:`compact` wrapper.


   .. py:attribute:: irreps
      :type:  e3nn_jax.Irreps


   .. py:attribute:: irreps_out
      :type:  e3nn_jax.Irreps
      :value: None



   .. py:attribute:: depth
      :type:  int
      :value: 1



   .. py:attribute:: norm_last
      :type:  bool
      :value: True



   .. py:attribute:: base
      :type:  flax.linen.Module


